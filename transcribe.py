import time
import os
import sys
import codecs
# import torch
import numpy as np
import onnxruntime as ort
import librosa
from pathlib import Path
from typing import Optional, List
# from transformers import WhisperFeatureExtractor, Qwen2TokenizerFast <-- Removed

# æ·»åŠ æœ¬åœ° Qwen-ASR-GGUF é€‚é…åº“è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.absolute()
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
from qwen_asr_gguf import llama

# ==================== Vulkan é€‰é¡¹ ====================

# os.environ["VK_ICD_FILENAMES"] = "none"       # ç¦æ­¢ Vulkan
# os.environ["GGML_VK_VISIBLE_DEVICES"] = "1"   # ç¦æ­¢ Vulkan ç”¨ç‹¬æ˜¾ï¼ˆå¼ºåˆ¶ç”¨é›†æ˜¾ï¼‰
# os.environ["GGML_VK_DISABLE_F16"] = "1"       # ç¦æ­¢ VulkanFP16 è®¡ç®—ï¼ˆIntelé›†æ˜¾fp16æœ‰æº¢å‡ºé—®é¢˜ï¼‰

providers = ['DmlExecutionProvider']
# providers = ['CPUExecutionProvider']


# ==========================================
# é…ç½®å‚æ•° (æ ¹æ®æ‚¨çš„è·¯å¾„ç¯å¢ƒä¿®æ”¹)
# ==========================================
FRONTEND_ONNX_PATH = os.path.join(PROJECT_ROOT, "model", "onnx", "qwen3_asr_encoder_frontend.fp32.onnx")
BACKEND_ONNX_PATH = os.path.join(PROJECT_ROOT, "model", "onnx", "qwen3_asr_encoder_backend.fp32.onnx")
LLM_GGUF_PATH = os.path.join(PROJECT_ROOT, "model", "qwen3_asr_llm.q8_0.gguf")

# Special Token IDs
ID_IM_START = 151644
ID_IM_END = 151645
ID_AUDIO_START = 151669
ID_AUDIO_END = 151670
ID_AUDIO_PAD = 151676
ID_ASR_TEXT = 151704

class FastWhisperMel:
    """å®Œå…¨åŸºäº NumPy å’Œ Librosa çš„ Mel æå–å™¨ (æ›¿ä»£ Transformers)"""
    def __init__(self, filter_path):
        self.filters = np.load(filter_path) # (201, 128)
        
    def __call__(self, audio):
        # 1. STFT (Reflect padding, Hann window)
        stft = librosa.stft(audio, n_fft=400, hop_length=160, window='hann', center=True)
        # 2. Power Spectrum
        magnitudes = np.abs(stft) ** 2 
        # 3. Mel Filterbank
        mel_spec = np.dot(self.filters.T, magnitudes) # (128, 201) x (201, T) -> (128, T)
        # 4. Log10
        log_spec = np.log10(np.maximum(mel_spec, 1e-10))
        # 5. Dynamic Range Compression
        log_spec = np.maximum(log_spec, np.max(log_spec) - 8.0)
        # 6. Scaling
        log_spec = (log_spec + 4.0) / 4.0
        return log_spec.T.astype(np.float32) # (T, 128)

class Qwen3ASRTranscriber:
    def __init__(self):
        t0 = time.time()
        print("--- åˆå§‹åŒ–éŸ³é¢‘è½¬å½•å¼•æ“ (æ¨¡å—åŒ– Encoder) ---")
        
        if str(PROJECT_ROOT) not in sys.path:
            sys.path.insert(0, str(PROJECT_ROOT))

        # 1. åŠ è½½ Mel æ»¤æ³¢å™¨
        print(f"åŠ è½½ Custom Mel Filters...")
        # å‡è®¾ filter åœ¨ model/mel_filters.npyï¼Œå¦‚æœä¸å­˜åœ¨éœ€è¦æç¤ºç”Ÿæˆ
        filter_path = os.path.join(PROJECT_ROOT, "model", "mel_filters.npy")
        if not os.path.exists(filter_path):
             raise FileNotFoundError(f"æ‰¾ä¸åˆ° Mel æ»¤æ³¢å™¨æ–‡ä»¶: {filter_path}ã€‚è¯·å…ˆè¿è¡Œ export_mel_filters.py")
        self.mel_extractor = FastWhisperMel(filter_path)
        
        
        # 2. åŠ è½½æ¨¡å—åŒ– Encoder ONNX
        
        sess_options = ort.SessionOptions()
        sess_options.log_severity_level = 3
        sess_options.add_session_config_entry("session.intra_op.allow_spinning", "0")
        sess_options.add_session_config_entry("session.inter_op.allow_spinning", "0")
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        print(f"åŠ è½½ Encoder Frontend: {FRONTEND_ONNX_PATH}")
        self.frontend_sess = ort.InferenceSession(FRONTEND_ONNX_PATH, sess_options=sess_options, providers=providers)
        print(f"åŠ è½½ Encoder Backend: {BACKEND_ONNX_PATH}")
        self.backend_sess = ort.InferenceSession(BACKEND_ONNX_PATH, sess_options=sess_options, providers=providers)
        
        # æ¨¡å‹é¢„çƒ­ (Warmup)
        dummy_mel = np.random.randn(1, 200, 128).astype(np.float32)
        dummy_feat = self.frontend_sess.run(None, {"mel": dummy_mel})[0]
        self.backend_sess.run(None, {"feat_in": dummy_feat})
        
        # 3. åŠ è½½ LLM GGUF
        print(f"åŠ è½½ GGUF LLM: {LLM_GGUF_PATH}")
        self.model = llama.LlamaModel(LLM_GGUF_PATH)
        self.embedding_table = llama.get_token_embeddings_gguf(LLM_GGUF_PATH)
        
        # æ›´æ–° Token IDs ä¸ºåŸç”ŸæŸ¥è¯¢ç»“æœ
        global ID_IM_START, ID_IM_END, ID_AUDIO_START, ID_AUDIO_END, ID_ASR_TEXT
        ID_IM_START = self.model.token_to_id("<|im_start|>")
        ID_IM_END = self.model.token_to_id("<|im_end|>")
        ID_AUDIO_START = self.model.token_to_id("<|audio_start|>")
        ID_AUDIO_END = self.model.token_to_id("<|audio_end|>")
        ID_ASR_TEXT = self.model.token_to_id("<asr_text>")

        # 4. åˆ›å»º Context å’Œ Sampler
        self.ctx = llama.LlamaContext(self.model, n_ctx=4096, n_batch=4096, embeddings=False)
        self.sampler = llama.LlamaSampler(temperature=0) 
        
        self.t_load_duration = time.time() - t0

    def transcribe(self, audio_path: str, context: str = "", language: str = None):
        if not os.path.exists(audio_path):
            print(f"âŒ æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            return
        print(f"--- Processing {audio_path} ---")
        
        # 1. æå– Mel ç‰¹å¾
        audio, _ = librosa.load(audio_path, sr=16000)
        mel = self.mel_extractor(audio) # [T, 128]
        mel = mel[np.newaxis, ...] # [1, T, 128]
            
        t_encoder_start = time.time()
        
        # 2. ç¼–ç å™¨å‰é¦ˆ
        # Step A: Frontend (å·ç§¯)
        t_front_start = time.time()
        feat_out = self.frontend_sess.run(None, {"mel": mel})[0]
        t_front_end = time.time()
        
        # Step B: Backend (Transformer)
        t_back_start = time.time()
        audio_embd = self.backend_sess.run(None, {"feat_in": feat_out})[0]
        if audio_embd.ndim == 3: audio_embd = audio_embd[0] # [T_ds, 1024]
        t_back_end = time.time()
        
        t_encoder_end = time.time()
        n_audio_tokens = audio_embd.shape[0]
        
        # 3. å‡†å¤‡ LLM Input Embeddings
        # Prompt æ ¼å¼:
        # <|im_start|>system\n...<|im_end|>
        # <|im_start|>user\n{context}\n\n<|audio_start|>{Audio Embeds}<|audio_end|>è¯­éŸ³è½¬å½•ï¼š<|im_end|>
        # <|im_start|>assistant\n
        
        user_prompt_text = ''
        if context:
            user_prompt_text = f"{context}\n\n"

        prefix_tokens = [ID_IM_START] + self.model.tokenize("system\nYou are a helpful assistant.") + [ID_IM_END] + \
                        [ID_IM_START] + self.model.tokenize(f"user\n{user_prompt_text}") + [ID_AUDIO_START]
        
        # æ„å»º Assistant å¼•å¯¼éƒ¨åˆ†
        # æ ¼å¼: <|audio_end|>è¯­éŸ³è½¬å½•ï¼š<|im_end|><|im_start|>assistant\n[language {Lang}]<asr_text>
        assistant_prompt = "assistant\n"
        if language:
            assistant_prompt += f"language {language}"
            
        suffix_tokens = [ID_AUDIO_END] + self.model.tokenize("è¯­éŸ³è½¬å½•ï¼š") + [ID_IM_END] + \
                        [ID_IM_START] + self.model.tokenize(assistant_prompt) + [ID_ASR_TEXT]
        
        n_prefix = len(prefix_tokens)
        n_suffix = len(suffix_tokens)
        total_len = n_prefix + n_audio_tokens + n_suffix
        
        full_embd = np.zeros((total_len, self.model.n_embd), dtype=np.float32)
        full_embd[:n_prefix] = self.embedding_table[prefix_tokens]
        full_embd[n_prefix : n_prefix + n_audio_tokens] = audio_embd
        full_embd[n_prefix + n_audio_tokens :] = self.embedding_table[suffix_tokens]
        
        # 4. Prefill (æ¨ç†)
        pos_base = np.arange(0, total_len, dtype=np.int32)
        pos_arr = np.concatenate([pos_base, pos_base, pos_base, np.zeros(total_len, dtype=np.int32)])
        
        # Qwen3 ä½¿ç”¨ M-RoPE ä½ç½®ç¼–ç ï¼Œæ¯ä¸ª token éœ€è¦ 4 ä¸ªä½ç½®å€¼ã€‚
        # å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åˆ†é… 4 å€çš„ Batch Pos ç©ºé—´ä»¥é˜²æ­¢ Heap Corruption (Buffer Overflow)ã€‚
        batch = llama.LlamaBatch(max(total_len * 4, 8192), self.model.n_embd, 1)
        batch.set_embd(full_embd, pos=pos_arr)
        
        print(f"DEBUG: Starting Prefill. total_len={total_len}, batch_size={max(total_len, 2048)}")
        self.ctx.clear_kv_cache()
        
        t_prefill_start = time.time()
        try:
            if self.ctx.decode(batch) != 0:
                print("âŒ Prefill å¤±è´¥ (decode returned non-zero)")
                return
        except Exception as e:
            print(f"âŒ Prefill å¼‚å¸¸: {e}")
            raise e
        t_prefill_end = time.time()
        
        print("DEBUG: Prefill successful. Starting Generation.")
            
        # 5. é€’å½’ç”Ÿæˆæ–‡å­—
        gen_batch = llama.LlamaBatch(4, 0, 1)
        cur_pos = total_len
        result_text = ""
        
        t_decode_start = time.time()
        n_decode_tokens = 0
        
        print("Result: ", end="", flush=True)
        
        # BPE å¢é‡è§£ç å™¨ (å¤„ç†å¤šå­—èŠ‚å­—ç¬¦åˆ†å‰²)
        decoder = codecs.getincrementaldecoder("utf-8")(errors='replace')
        
        for i in range(512):
            token_id = self.sampler.sample(self.ctx.ptr)
            if token_id == self.model.eos_token or token_id == ID_IM_END:
                break
            
            # ä¼˜å…ˆæäº¤ä¸‹ä¸€æ¬¡è§£ç ä»»åŠ¡ (Compute First)
            gen_batch.set_token(token_id, pos=np.array([cur_pos, cur_pos, cur_pos, 0], dtype=np.int32))
            ret = self.ctx.decode(gen_batch)
            cur_pos += 1
            n_decode_tokens += 1
            
            # åå¤„ç†è€—æ—¶æ“ä½œ (IO / Text Processing)
            # è·å– Token å­—èŠ‚å¹¶æµå¼è§£ç 
            delta = decoder.decode(self.model.token_to_bytes(token_id), final=False)
            if delta:
                print(delta, end="", flush=True)
                result_text += delta
            
            # æ£€æŸ¥è§£ç ç»“æœ
            if ret != 0:
                print(f"âŒ Decode Error: {ret}")
                break
        
        
        t_decode_end = time.time()
            
        print("\n--- è½¬å½•ç»“æŸ ---")
        
        # ç»Ÿè®¡æŠ¥å‘Š
        t_encoder_total = t_encoder_end - t_encoder_start
        t_prefill_total = t_prefill_end - t_prefill_start
        t_decode_total = t_decode_end - t_decode_start
        
        # è®¡ç®—æ€»è€—æ—¶ (ä» Encoder å¼€å§‹åˆ° Decode ç»“æŸ)
        t_total_transcribe = t_encoder_total + t_prefill_total + t_decode_total
        
        # è·å–éŸ³é¢‘æ—¶é•¿ (seconds)
        audio_duration = len(audio) / 16000
        rtf = t_total_transcribe / audio_duration if audio_duration > 0 else 0
        
        prefill_cps = total_len / t_prefill_total if t_prefill_total > 0 else 0
        decode_cps = n_decode_tokens / t_decode_total if t_decode_total > 0 else 0
        
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  ğŸ”¹ åŠ è½½è€—æ—¶: {self.t_load_duration:.3f} s")
        print(f"  ğŸ”¹ éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} s")
        print(f"  ğŸ”¹ æ€»è€—æ—¶  : {t_total_transcribe:.3f} s (RTF: {rtf:.4f})")
        print(f"  ğŸ”¹ - éŸ³é¢‘ç¼–ç : {t_encoder_total:.3f} s (Frontend: {t_front_end - t_front_start:.3f}s, Backend: {t_back_end - t_back_start:.3f}s)")
        print(f"  ğŸ”¹ - Prefill : {t_prefill_total:.3f} s | {total_len} tokens | {prefill_cps:.1f} tokens/s")
        print(f"  ğŸ”¹ - Decode  : {t_decode_total:.3f} s | {n_decode_tokens} tokens | {decode_cps:.1f} tokens/s")
        
        return result_text

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Qwen3-ASR (Modular ONNX + GGUF) ç¦»çº¿è½¬å½•å·¥å…·")
    parser.add_argument("input", help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (å¦‚ test.mp3)")
    parser.add_argument("--context", help="è½¬å½•ä¸Šä¸‹æ–‡ä¿¡æ¯ (æœ‰åŠ©äºæé«˜å‡†ç¡®åº¦)", default=None)
    parser.add_argument("--language", help="å¼ºåˆ¶æŒ‡å®šè¯­è¨€ (å¦‚ 'Chinese', 'English')", default=None)
    args = parser.parse_args()
    
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    engine = Qwen3ASRTranscriber()
    engine.transcribe(args.input, context=args.context, language=args.language)
